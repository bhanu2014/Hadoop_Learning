package com.spark.learning;

import java.util.List;

import org.apache.spark.SparkConf;
import org.apache.spark.api.java.JavaRDD;
import org.apache.spark.api.java.JavaSparkContext;

import scala.Tuple2;

public class BasicLogAnalyser {

	public static void main(String[] args) {
		// TODO Auto-generated method stub
			SparkConf conf = new SparkConf().setAppName("Basic log analyser").setMaster("local");
			JavaSparkContext contx=new JavaSparkContext(conf);
			
			String logFile="C:/Users/Vostro/Desktop/input/input.txt";
			
			JavaRDD<String> logRdd=contx.textFile(logFile).cache();
			//converting raw RDD to RDD of ApacheAcessLog objects
			JavaRDD<ApacheAccessLog> acesslog =logRdd.map(FUNCTIONS.PARSE_LOG_LINE);
			
			
			//Computing avaerage,min,max content sizes
		
			JavaRDD<Long> contentSize=acesslog.map(FUNCTIONS.GET_CONTENT_SIZE);
			
			System.out.println("Average Content Size: "+contentSize.reduce(FUNCTIONS.SUM_REDUCER)/contentSize.count());
			System.out.println("Minimun Content Size: "+contentSize.min(FUNCTIONS.LONG_NATURAL_ORDER_COMPARATOR));
			System.out.println("Maximum ContentSize: "+contentSize.max(FUNCTIONS.LONG_NATURAL_ORDER_COMPARATOR));
			
			List<Tuple2<Integer,Long>> responseCodeToCount=acesslog.mapToPair(FUNCTIONS.GET_RESPONSE_CODE)
																	.reduceByKey(FUNCTIONS.SUM_REDUCER)
																	.take(100);
			
			System.out.println("Response codes count: "+responseCodeToCount);
			
			
			
			
	}

}
