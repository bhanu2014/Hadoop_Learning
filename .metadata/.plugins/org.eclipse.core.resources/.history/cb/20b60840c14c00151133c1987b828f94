package com.spark.streaming;

import org.apache.spark.SparkConf;
import org.apache.spark.api.java.JavaSparkContext;
import org.apache.spark.api.java.function.Function;
import org.apache.spark.streaming.Duration;
import org.apache.spark.streaming.api.java.JavaDStream;
import org.apache.spark.streaming.api.java.JavaReceiverInputDStream;
import org.apache.spark.streaming.api.java.JavaStreamingContext;

import com.spark.learning.ApacheAccessLog;

public class LogAnalyzerStreamingSQL {
	  private static final Duration WINDOW_LENGTH = new Duration(30 * 1000);
	  // Stats will be computed every slide interval time.
	  private static final Duration SLIDE_INTERVAL = new Duration(10 * 1000);
	@SuppressWarnings("resource")
	public static void main(String[] args){
		SparkConf conf=new SparkConf().setAppName("Spark Streaming learning").setMaster("local");
		JavaSparkContext spctx=new JavaSparkContext(conf);
		JavaStreamingContext jssc=new JavaStreamingContext(spctx,SLIDE_INTERVAL);
		JavaReceiverInputDStream<String> logDataStream=jssc.socketTextStream("localhost",9999);
		JavaDStream<CountryBean> accesslogDstream=logDataStream.map(new Function<String, CountryBean>() {

			private static final long serialVersionUID = 1L;

			public CountryBean call(String logline){
				return CountryBean.parseStreamLine(line);
			}
		}).filter(new Function<CountryBean, Boolean>() {
			private static final long serialVersionUID = 1L;
			public Boolean call(CountryBean bean) throws Exception {
				if(bean == null) {
					return false;
				}
				return true;
			}
		});
		JavaDStream<CountryBean> windowDstream=accesslogDstream.window(WINDOW_LENGTH,SLIDE_INTERVAL);
		
		windowDstream.foreach(accesslogs ->{
			if(accesslogs.count()==0){
				System.out.println("No acesslogs in this interval");
			}
			else{
				System.out.println("its working: Count:"+accesslogs.count());
			}
			
			
			return null;
		});
		jssc.start();
		jssc.awaitTermination();
	}
}
