package com.spark.hbase;


import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.hbase.HBaseConfiguration;
import org.apache.spark.SparkConf;
import org.apache.spark.api.java.JavaRDD;
import org.apache.spark.api.java.JavaSparkContext;
import org.apache.spark.api.java.function.Function;
import org.apache.spark.api.java.function.Function2;

//import com.spark.learning.ApacheAccessLog;

public class SparkHbaseBulkInsert {
	
	@SuppressWarnings({ "resource", "serial" })
	public static void main(String[] args){
		SparkConf conf=new SparkConf().setAppName("spark hbase bulk insert").setMaster("local[4]");
		JavaSparkContext ctx=new JavaSparkContext(conf);
		
		JavaRDD<String> logData=ctx.textFile("input_data/input.txt");
		
		System.out.println("log data count"+logData.count());
		
		JavaRDD<ApacheAccessLog> accessLogRDD=logData.map(new Function<String, ApacheAccessLog>() {
			public ApacheAccessLog call(String line){
				return ApacheAccessLog.parseFromLogLine(line);
			}
		}).filter(new Function<ApacheAccessLog, Boolean>() {
			private static final long serialVersionUID = 1L;
			public Boolean call(ApacheAccessLog bean) throws Exception {
				if(bean == null) {
					return false;
				}
				return true;
			}
		});
		
		
		
		Configuration hconf=new HBaseConfiguration().create();
	}
}
