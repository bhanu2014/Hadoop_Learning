package com.bimarianDev.truckDataIngestion;

import java.io.FileNotFoundException;
import java.io.Serializable;
import java.util.List;

import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.hbase.HBaseConfiguration;
import org.apache.hadoop.hbase.client.Put;
import org.apache.hadoop.hbase.client.Row;
import org.apache.hadoop.hbase.util.Bytes;
import org.apache.spark.SparkConf;
import org.apache.spark.SparkContext;
import org.apache.spark.api.java.JavaRDD;
import org.apache.spark.api.java.JavaSparkContext;
import org.apache.spark.api.java.function.Function;
import org.apache.spark.api.java.function.PairFunction;
import org.apache.spark.rdd.RDD;
import org.apache.spark.sql.SQLContext;
import org.apache.spark.sql.api.java.JavaSQLContext;
import org.apache.spark.sql.api.java.JavaSchemaRDD;

import scala.Function1;
import scala.Tuple2;

import com.cloudera.spark.hbase.JavaHBaseContext;
//import com.spark.hbase.SparkHbaseBulkInsert.PutFunction;
import com.google.gson.JsonIOException;
import com.google.gson.JsonSyntaxException;

@SuppressWarnings("serial")
public class TruckDataBulkInsert implements Serializable{
	@SuppressWarnings({ })
	public static void main(String[] args) throws JsonIOException, JsonSyntaxException, FileNotFoundException{
		SparkConf conf=new SparkConf().setAppName("Truck Data Bulk Insert").setMaster("local");
		JavaSparkContext ctx=new JavaSparkContext(conf);
		String path="C:/Users/Vostro/Desktop/input/employee.json";
		JavaSQLContext sqlContext = new JavaSQLContext(ctx);
		JavaSchemaRDD  truck=sqlContext.jsonFile(path);
		truck.printSchema();
		truck.registerTempTable("trucks");
		sqlContext.sqlContext().cacheTable("trucks");
		
		JavaSchemaRDD rs= sqlContext.sql("select VNo from trucks");
		List<String> vns=rs.map(new Function<Row, String>() {

			@Override
			public String call(Row arg0) throws Exception {
				// TODO Auto-generated method stub
				return arg0.getString[0];
			}
		}).collect();
		
		
//		Tuple4<Long, Long, Long, Long> contentSizeStats =
//			    sqlContext.sql("SELECT SUM(contentSize), COUNT(*), MIN(contentSize), MAX(contentSize) FROM logs")
//			        .map(row -> new Tuple4<>(row.getLong(0), row.getLong(1), row.getLong(2), row.getLong(3)))
//			        .first();
//			System.out.println(String.format("Content Size Avg: %s, Min: %s, Max: %s",
//			    contentSizeStats._1() / contentSizeStats._2(),
//			    contentSizeStats._3(),
//			    contentSizeStats._4()));
//		
		
		
		
		
		
		
		
		
		
		
		
		
		
		
		
		
		
		
		
		
		
		
		
		
		
		
		
		
		//JavaRDD<String> truckData=ctx.textFile("hdfs://hadoop1.test.com/user/root/bhanu/countryLog.txt");
//		JavaRDD<TruckBean> truckinnfo=TruckJsonToObject.getTruckData("C:/Users/Vostro/Desktop/input/employee.json");
//		
//		System.out.println("log data count"+truckinnfo.count());

		
	
//		Configuration hconf = HBaseConfiguration.create();
//		hconf.addResource(new Path("/etc/hbase/conf.cloudera.yarn/core-site.xml"));
//		hconf.addResource(new Path("/etc/hbase/conf.cloudera.hbase/hbase-site.xml"));
		
//		JavaHBaseContext hbaseContext =new JavaHBaseContext(ctx,hconf);
//		
//		//hbaseContext.bulkPut(truckData,"truckInfo",new PutFunction(), true);
//		}
//		@SuppressWarnings("serial")
//		public static class PutFunction implements Function<String, Put>{
//			@Override
//			public Put call(String logline) throws Exception {
//				String[] cell=logline.split(",");
//				 Put put = new Put(Bytes.toBytes(System.currentTimeMillis()));
//				 put.add((Bytes.toBytes("truckParameters")),(Bytes.toBytes("VNo")),(Bytes.toBytes( cell[0])));
//				 put.add((Bytes.toBytes("truckParameters")),(Bytes.toBytes("groupId")),(Bytes.toBytes( cell[1])));
//				 put.add((Bytes.toBytes("truckParameters")),(Bytes.toBytes("deploymentId")),(Bytes.toBytes( cell[2])));
//				 put.add((Bytes.toBytes("truckParameters")),(Bytes.toBytes("Time")),(Bytes.toBytes( cell[3])));
//				 put.add((Bytes.toBytes("truckParameters")),(Bytes.toBytes("Acc")),(Bytes.toBytes( cell[4])));
//				 put.add((Bytes.toBytes("truckParameters")),(Bytes.toBytes("Lat")),(Bytes.toBytes( cell[5])));
//				 put.add((Bytes.toBytes("truckParameters")),(Bytes.toBytes("Lon")),(Bytes.toBytes( cell[6])));
//				 put.add((Bytes.toBytes("truckParameters")),(Bytes.toBytes("Speed")),(Bytes.toBytes( cell[7])));
//				 put.add((Bytes.toBytes("truckParameters")),(Bytes.toBytes("Angle")),(Bytes.toBytes( cell[8])));
//				 put.add((Bytes.toBytes("truckParameters")),(Bytes.toBytes("Locate")),(Bytes.toBytes( cell[9])));
//				 put.add((Bytes.toBytes("truckParameters")),(Bytes.toBytes("Oil")),(Bytes.toBytes( cell[10])));
//				 put.add((Bytes.toBytes("truckParameters")),(Bytes.toBytes("Weight")),(Bytes.toBytes( cell[11])));
//				 put.add((Bytes.toBytes("truckParameters")),(Bytes.toBytes("Mile")),(Bytes.toBytes( cell[12])));
//				 put.add((Bytes.toBytes("truckParameters")),(Bytes.toBytes("version")),(Bytes.toBytes( cell[13])));
//				 put.add((Bytes.toBytes("truckParameters")),(Bytes.toBytes("timestamp")),(Bytes.toBytes( cell[14])));
//				 put.add((Bytes.toBytes("truckParameters")),(Bytes.toBytes("Location[0]")),(Bytes.toBytes( cell[15])));
//				 put.add((Bytes.toBytes("truckParameters")),(Bytes.toBytes("Location[1]")),(Bytes.toBytes( cell[16])));
//				 
//				return put;
//			}
//			
		}
}
