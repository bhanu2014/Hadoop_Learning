package com.spark.learning;

import java.util.List;

import org.apache.spark.SparkConf;
import org.apache.spark.api.java.JavaRDD;
import org.apache.spark.api.java.JavaSparkContext;
import org.apache.spark.api.java.function.Function;
import org.apache.spark.api.java.function.Function2;

import com.google.common.base.Functions;

import scala.Tuple2;

public class BasicLogAnalyser {

	@SuppressWarnings("serial")
	public static void main(String[] args) {
		// TODO Auto-generated method stub
			SparkConf conf = new SparkConf().setAppName("Basic log analyser").setMaster("local");
			@SuppressWarnings("resource")
			JavaSparkContext contx=new JavaSparkContext(conf);
			
			String logFile="C:/Users/Vostro/Desktop/input/input.txt";
			
			JavaRDD<String> logRdd=contx.textFile(logFile).cache();
			//converting raw RDD to RDD of ApacheAcessLog objects
//			JavaRDD<ApacheAccessLog> acesslog =logRdd.map(FUNCTIONS.PARSE_LOG_LINE)
//												.filter(FUNCTIONS.EXCEPTION_CATCH);
//			
			JavaRDD<ApacheAccessLog> acesslog=logRdd.map(new Function<String, ApacheAccessLog>() {
				
																private static final long serialVersionUID = 1L;

															public ApacheAccessLog call(String logline){
																return ApacheAccessLog.parseFromLogLine(logline);
															}
														}).filter(new Function<ApacheAccessLog, Boolean>() {
															private static final long serialVersionUID = 1L;
															public Boolean call(ApacheAccessLog bean) throws Exception {
																if(bean == null) {
																	return false;
																}
																return true;
															}
													 	});
			//
			
			//Computing avaerage,min,max content sizes
		
			JavaRDD<Long> contentSize=acesslog.map(new Function<ApacheAccessLog, Long>() {

												private static final long serialVersionUID = 1L;
								
												public Long call(ApacheAccessLog acesslog){
													return acesslog.getContentSize();
												}
											});

			System.out.println("Average Content Size: "+contentSize.reduce(new Function2<Long, Long, Long>() {
																				public Long call(Long a,Long b){
																					return a+b;
																				}
																			})/contentSize.count());
			System.out.println("Minimun Content Size: "+contentSize.min(FUNCTIONS.LONG_NATURAL_ORDER_COMPARATOR));
			System.out.println("Maximum ContentSize: "+contentSize.max(FUNCTIONS.LONG_NATURAL_ORDER_COMPARATOR));
			
			List<Tuple2<Integer,Long>> responseCodeToCount=acesslog.mapToPair(FUNCTIONS.GET_RESPONSE_CODE)
																	.reduceByKey(FUNCTIONS.SUM_REDUCER)
																	.take(100);
			
			System.out.println("Response codes count: "+responseCodeToCount);
			
			
			
			
			//Computing top 10 ip address
			
			List<String> ipAddress =acesslog.mapToPair(FUNCTIONS.GET_IP_ADDRESS)
									.reduceByKey(FUNCTIONS.SUM_REDUCER)
									.filter(FUNCTIONS.GREATER_THAN_10)
									.map(FUNCTIONS.GET_TUPLE_FIRST)
									.take(100);
			
			System.out.println("Ip addresses: "+ipAddress);
			//Top end points
			
			List<Tuple2<String,Long>> topEndPoints=acesslog.mapToPair(FUNCTIONS.GET_END_POINT)
															.reduceByKey(FUNCTIONS.SUM_REDUCER)
															 .top(10, new FUNCTIONS.ValueComparator<String, Long> (FUNCTIONS.LONG_NATURAL_ORDER_COMPARATOR));
			
			
			System.out.println("Top end points: "+topEndPoints);
			contx.stop();
	}

}
