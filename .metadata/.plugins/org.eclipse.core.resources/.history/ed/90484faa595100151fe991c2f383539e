package com.spark.hbase;

import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.hbase.Cell;
import org.apache.hadoop.hbase.HBaseConfiguration;
import org.apache.hadoop.hbase.KeyValue;
import org.apache.hadoop.hbase.client.Get;
import org.apache.hadoop.hbase.client.Result;
import org.apache.hadoop.hbase.util.Bytes;
import org.apache.spark.SparkConf;
import org.apache.spark.api.java.JavaRDD;
import org.apache.spark.api.java.JavaSparkContext;
import org.apache.spark.api.java.function.Function;

import java.util.Iterator;

import akka.dispatch.Foreach;

import com.cloudera.spark.hbase.JavaHBaseContext;

public class SparkHbaseBulkGet {
	public static void  main(String[] args){
		SparkConf conf=new SparkConf().setAppName("Spark with hbase bulk get").setMaster("local[4]");
		JavaSparkContext ctx=new JavaSparkContext(conf);
		Configuration hconf=HBaseConfiguration.create();
		hconf.addResource(new Path("/etc/hbase/conf.cloudera.yarn/core-site.xml"));
		hconf.addResource(new Path("/etc/hbase/conf.cloudera.hbase/hbase-site.xml"));
		JavaHBaseContext hctx=new JavaHBaseContext(ctx, hconf);
		JavaRDD<byte[]> rdd=null;
		hctx.bulkGet("countryLog", 3,rdd , new GetFunction(), new ResultFunction());
	}
	
	@SuppressWarnings("serial")
	public static class GetFunction implements Function<byte[],Get>{

		public Get call(byte[] v) throws Exception {
			// TODO Auto-generated method stub
			return new Get(v);
		}
	}
	
	public static class ResultFunction implements Function<Result,String>{
		private static final long serialVersionUID = 1L;

		@Override
		public String call(Result result) throws Exception {
			@SuppressWarnings("unused")
			
			Iterator<Cell> it=result.listCells().iterator();
			for(Cell c:result.listCells()){
				System.out.println(Bytes.toString(c.getFamilyArray()));
				c.getQualifierArray();
				c.getValueArray();
			}
			return null;
		}
		
	}
}
